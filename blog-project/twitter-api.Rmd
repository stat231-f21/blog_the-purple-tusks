---
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(csv)
library(dplyr)
library(rtweet)
library(httpuv)
library(tidytext)
library(lubridate)
library(twitteR)
library(robotstxt)
library(readxl)
library(wordcloud)
library(ggwordcloud)
```



```{r, authentification-token}
# store api keys
api_key <- "1zTkAFaSLvGEUnB2PPPe9nSqh"
api_secret_key <- "1T0nQLQwSUZVqAPhfn3QB2TEy5X81ay8sUjAzNOSVLpEO92gMM"
access_token <- "1269483564384444416-S4blcCi966TkuH8XKyboBD1OvlQQLk"
access_token_secret <- "5vwTUNgZPiFdqyve3SKjsZm11vdNQFac46uEDXsG1vcGq"

# authenticate via web broswer
token <- create_token(
  app = "blog-project",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)

consumer_key = api_key
consumer_secret = api_secret_key
access_token = access_token
access_secret = access_token_secret

saveRDS(token, "~/.rtweet.rds")
twitter_pat = "~/.rtweet.rds"

get_token()
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```

```{r twitter-senators-links}
#get state abbreviations
state_full <- tibble(abv = state.abb,
                     state = state.name)

#read from excel, source: https://www.autismspeaks.org/sites/default/files/116th%20Senator%20Twitter%20Handles.pdf
senator_profiles_117 <- read_xlsx("data/textual/congress_117.xlsx")
#tidy up
senator_profiles_117 <- senator_profiles_117 %>%
  rename(name = "SENATORS", link = "...2", abv = "...3", party = "...4") %>%
  filter(!(name=="Name")) %>%
  #add state names
  inner_join(state_full, by = c("abv" = "abv")) %>%
  #add congress year, tidy username
  mutate(congress = 117, 
         link = substring(link,21)) %>%
  select(-party)

#reorder name order (first -> last)

for(i in 1:nrow(senator_profiles_117)){
  if(i != 55){
    location <- as.numeric(gregexec(pattern = ",", senator_profiles_117$name[i]))
    first = substring(text = senator_profiles_117$name[i], location + 2)
    last = substring(text = senator_profiles_117$name[i], first = 1, last = location-1)
    senator_profiles_117$name[i] = paste(first, last)}
  else if (i == 55){
    senator_profiles_117$name[i] = "Joe Manchin III"
  }
}

#116th congress
senator_profiles_116 <- read_file("data/textual/congress_116.txt")

#organice text file
senator_profiles_116 <- read_lines(senator_profiles_116) %>%
  data_frame() %>%
  rename(state = ".")

temp <- str_split_fixed(senator_profiles_116$state, " ", 2)

temp2 <- str_split_fixed(temp[,2], " @", 2)

#create dataframe
senator_profiles_116 <- tibble(abv = temp[,1], name = temp2[,1], link = temp2[,2]) %>%
  inner_join(state_full, by = c("abv" = "abv")) %>%
  mutate(congress = 116)

rm(temp,temp2)

#combine
senator_profiles <- rbind(senator_profiles_116,senator_profiles_117) %>%
  arrange(by = link) %>%
  filter(duplicated(link) == FALSE)

#excepts
senator_profiles$link[32] <- "LeaderMcConnell"
senator_profiles$link[51] <- "RepMarshall"
senator_profiles$link[86] <- "KamalaHarris"
senator_profiles$link[91] <- "MarthaMcSally"
senator_profiles <- senator_profiles[-33,]
#Marshall has no tweets
senator_profiles <- senator_profiles[-51,]

#test code

senator_profiles
```

```{r twitter-senator-scraping, eval=FALSE}
#initialize dataset
senator_tweets = data_frame()

#calculate run time
start <- Sys.time()

#loop to get twitter timelines for all senators of past two years
for(i in 91:nrow(senator_profiles)){
  profile_start <- Sys.time()
  
  tweets <- get_timeline(user = senator_profiles$link[i], n = 3200, check = FALSE) %>%
    #select relevant info
    select(c("created_at","screen_name","text","source","favorite_count","retweet_count","hashtags","location")) %>%
    #add info from senator tweets
    mutate(name = senator_profiles$name[i],
           abv = senator_profiles$abv[i],
           state = senator_profiles$state[i]) %>%
    #reorder columns
    select(c(10,11,9,3,4,5,6,7,1,2,8))
  
  senator_tweets <- rbind(senator_tweets, tweets)
  #see progress
  print(i)
  print(senator_profiles$name[i])
  print("profile time:")
  print(Sys.time() - profile_start)
  print("total:")
  print(Sys.time() - start)
}

write_csv2(senator_tweets,file = "data/textual/senator_tweets.csv")

finish <- Sys.time()

```

```{r tweet-sentiment}
#get stop words
data(stop_words)

#load data (NOTE: NEED TO UNZIP FILE FIRST FROM FOLDER)
senator_tweets <- read.csv2("data/textual/senator_tweets.csv")

#list of related words
related_words <- c("covid","mask.","mandate","vaccine","lockdown","pandemic","hospitlization","corona.","health.","case.","safety","distanc.","social","CDC","sick.","virus","quarantine","spread","contain","law","contact","isolation")

query <- paste(related_words, collapse = "|")

#filter through related tweets
senator_tweets_covid <- senator_tweets %>% 
  filter(grepl(query,text))

#wrangle the date
senator_tweets_covid <- senator_tweets_covid %>%
  rename(date = created_at) %>%
  #select date range
  mutate(date = ymd(substring(date, first = 1, last = 10)),
         week = 0) %>%
  filter(date %in% ymd("2020-04-23"):ymd("2021-10-11")) %>%
  arrange(date)

#add in week

#get the endpoints of our date divisions
text_data <- " 2020-04-23 - 2020-05-05
 2020-05-07 - 2020-05-12
 2020-05-14 - 2020-05-19
 2020-05-21 - 2020-05-26
 2020-05-28 - 2020-06-02
 2020-06-04 - 2020-06-09
 2020-06-11 - 2020-06-16
 2020-06-18 - 2020-06-23
 2020-06-25 - 2020-06-30 
 2020-07-02 - 2020-07-07
 2020-07-09 - 2020-07-14
 2020-07-16 - 2020-07-21
 2020-08-19 - 2020-08-31
 2020-09-02 - 2020-09-14
 2020-09-16 - 2020-09-28
 2020-09-30 - 2020-10-12
 2020-10-14 - 2020-10-26
 2020-10-28 - 2020-11-09
 2020-11-11 - 2020-11-23
 2020-11-25 - 2020-12-07
 2020-12-09 - 2020-12-21 
 2021-01-06 - 2021-01-18
 2021-01-20 - 2021-02-01 
 2021-02-03 - 2021-02-15
 2021-02-17 - 2021-03-01
 2021-03-03 - 2021-03-15
 2021-03-17 - 2021-03-29
 2021-04-14 - 2021-04-26
 2021-04-28 - 2021-05-10
 2021-05-12 - 2021-05-24 
 2021-05-26 - 2021-06-07
 2021-06-09 - 2021-06-21
 2021-06-23 - 2021-07-05 
 2021-07-21 - 2021-08-02
 2021-08-04 - 2021-08-16
 2021-08-18 - 2021-08-30
 2021-09-01 - 2021-09-13
 2021-09-15 - 2021-09-27
 2021-09-29 - 2021-10-11"
text_data <- text_data %>%
  str_split("\n| - ")

dates_data <- tibble(date = text_data) %>%
  unnest()


#check if the dates of tweets are within the range
for(i in 1:nrow(senator_tweets_covid)){
  for(j in 1:39){
    if(senator_tweets_covid$date[i] %in% ymd(dates_data$date[1+2*(j-1)]):ymd(dates_data$date[2*j])){
      senator_tweets_covid$week[i] = j
      break;
    }
  }
  print(i*100/nrow(senator_tweets_covid))
}

write_csv2(senator_tweets_covid, "data/textual/senator_tweets_covid.csv")
senator_tweets_covid <- read_csv2("data/textual/senator_tweets_covid.csv")

#tokenize the tweets
senator_covid_words <- senator_tweets_covid %>%
  unnest_tokens(input = text, output = tokens) %>% 
  #get rid of stop words
  anti_join(stop_words, by = c("tokens" = "word")) %>%
  #join with sentiments
  inner_join(get_sentiments(lexicon = "nrc"), by = c("tokens" = "word"))

#count the number of sentiments
word_count <- senator_covid_words %>%
  group_by(week,sentiment,state) %>%
  summarise(total = n()) %>%
  pivot_wider(names_from = sentiment, values_from = total, values_fill = 0) %>%
  mutate(total_tweets = positive+negative+anger+anticipation+disgust+fear+joy+sadness+surprise+trust) %>%
  pivot_longer(-c(week,total_tweets, state),names_to = "sentiments", values_to = "count") %>% 
  mutate(percentage = count / total_tweets,
         percentage_spatial = )

temp <- senator_covid_words %>%
  group_by(week,sentiment,state) %>%
  summarise(total = n()) %>%
  pivot_wider(names_from = sentiment, values_from = total, values_fill = 0) %>%
  mutate(total_tweets = positive+negative+anger+anticipation+disgust+fear+joy+sadness+surprise+trust) %>%
  pivot_longer(-c(week,total_tweets, state),names_to = "sentiments", values_to = "count") %>%
  group_by(state, sentiments) %>%
  summarise(total_sentiment = sum(count))

word_count <- inner_join(word_count, temp, by = c("state","sentiments")) %>%
  mutate(spatial_percentage = count * 100 / total_sentiment)

word_count_national <- senator_covid_words %>%
  group_by(week,sentiment) %>%
  summarise(total = n()) %>%
  pivot_wider(names_from = sentiment, values_from = total, values_fill = 0) %>%
  mutate(total_tweets = positive+negative+anger+anticipation+disgust+fear+joy+sadness+surprise+trust) %>%
  pivot_longer(-c(week,total_tweets),names_to = "sentiments", values_to = "count")

write_csv2(word_count, "data/textual/twitter_sentiments.csv")

write_csv2(word_count_national, "data/textual/ntl_twitter_sentiments.csv")

#test code

word_count %>%
  filter(state == "California") %>%
  ggplot(aes(x = week, y = count / sum)) +
  geom_line(aes(color = sentiments))

word_count_national %>%
  filter(!week == 0) %>% 
  ggplot(aes(x = week, y = count / sum)) +
  geom_col(aes(fill = sentiments))

```

```{r wordcloud}
#word cloud dataset with afinn
temp <- senator_tweets_covid %>%
  unnest_tokens(input = text, output = tokens) %>% 
  #get rid of stop words
  anti_join(stop_words, by = c("tokens" = "word"))

#create a dataset of word counts with 
senator_wordcloud <- tibble()

for(i in 1:39){
  for(j in 1:50){
    temp2 <- temp %>%
      filter(week == i, state == state.name[j]) %>%
      count(wt = favorite_count, tokens) %>%
      #join with sentiments
      inner_join(get_sentiments(lexicon = "afinn"), by = c("tokens" = "word")) %>%
      arrange(desc(n)) %>%
      mutate(week = i, state = state.name[j])
  
    senator_wordcloud <- senator_wordcloud %>%
      rbind(temp2)
  }
}

write_csv2(senator_wordcloud, "data/textual/senator_wordcloud.csv")

#create a total one
senator_wordcloud_total <- senator_tweets_covid %>%
  unnest_tokens(input = text, output = tokens) %>% 
  #get rid of stop words
  anti_join(stop_words, by = c("tokens" = "word")) %>%
  filter(week == 5) %>%
  count(wt = favorite_count, tokens) %>%
  #join with sentiments
  inner_join(get_sentiments(lexicon = "afinn"), by = c("tokens" = "word")) %>%
  arrange(desc(n))

write_csv2(senator_wordcloud_total, "data/textual/senator_wordcloud_total.csv")

ggplot(senator_wordcloud[1:50,], aes(label = tokens)) +
  geom_text_wordcloud(aes(size = n, color = value)) + 
  theme_minimal()

```


#draft code

<!-- ```{r tweet-miner, eval=FALSE} -->
<!-- #set the weeks character strings needed in the correct format -->
<!-- text_data <- " 2020-04-23 - 2020-05-05 -->
<!--  2020-05-07 - 2020-05-12 -->
<!--  2020-05-14 - 2020-05-19 -->
<!--  2020-05-21 - 2020-05-26 -->
<!--  2020-05-28 - 2020-06-02 -->
<!--  2020-06-04 - 2020-06-09 -->
<!--  2020-06-11 - 2020-06-16 -->
<!--  2020-06-18 - 2020-06-23 -->
<!--  2020-06-25 - 2020-06-30  -->
<!--  2020-07-02 - 2020-07-07 -->
<!--  2020-07-09 - 2020-07-14 -->
<!--  2020-07-16 - 2020-07-21 -->
<!--  2020-08-19 - 2020-08-31 -->
<!--  2020-09-02 - 2020-09-14 -->
<!--  2020-09-16 - 2020-09-28 -->
<!--  2020-09-30 - 2020-10-12 -->
<!--  2020-10-14 - 2020-10-26 -->
<!--  2020-10-28 - 2020-11-09 -->
<!--  2020-11-11 - 2020-11-23 -->
<!--  2020-11-25 - 2020-12-07 -->
<!--  2020-12-09 - 2020-12-21  -->
<!--  2021-01-06 - 2021-01-18 -->
<!--  2021-01-20 - 2021-02-01  -->
<!--  2021-02-03 - 2021-02-15 -->
<!--  2021-02-17 - 2021-03-01 -->
<!--  2021-03-03 - 2021-03-15 -->
<!--  2021-03-17 - 2021-03-29 -->
<!--  2021-04-14 - 2021-04-26 -->
<!--  2021-04-28 - 2021-05-10 -->
<!--  2021-05-12 - 2021-05-24  -->
<!--  2021-05-26 - 2021-06-07 -->
<!--  2021-06-09 - 2021-06-21 -->
<!--  2021-06-23 - 2021-07-05  -->
<!--  2021-07-21 - 2021-08-02 -->
<!--  2021-08-04 - 2021-08-16 -->
<!--  2021-08-18 - 2021-08-30 -->
<!--  2021-09-01 - 2021-09-13 -->
<!--  2021-09-15 - 2021-09-27 -->
<!--  2021-09-29 - 2021-10-11" -->
<!-- text_data <- text_data %>% -->
<!--   str_split("\n|  ") -->

<!-- dates_data <- tibble(date = text_data) %>% -->
<!--   unnest() -->

<!-- #grab twitter data -->

<!-- tweets_full <- tibble() -->

<!-- for(i in 1:39){ -->
<!--   for(j in 1:50){ -->
<!--     #query text with state name -->
<!--     query <- paste(state.name[j], -->
<!--                    " (covid OR mask OR mandate OR vaccine OR lockdown) lang:en") -->
<!--     print(query) -->

<!--     #the date parameter for search function -->
<!--     d1 <- paste(dates_data$date[1+2*(i-1)], -->
<!--                 "0101", -->
<!--                 sep = "") -->
<!--     d2 <- paste(dates_data$date[2*i], -->
<!--                 "0101", -->
<!--                 sep = "") -->

<!--     #search the twitter archive -->
<!--     temp <- search_fullarchive(q = query,  -->
<!--                                n = 500, -->
<!--                                fromDate = d1, -->
<!--                                toDate = d2, -->
<!--                                env_name = "historicalsearch", -->
<!--                                token = token) %>% -->
<!--       mutate(week = i, state = state.name[j]) %>% -->
<!--       select(c("screen_name","text","is_retweet","favorite_count","hashtags","name","location","followers_count","week","state")) -->

<!--     tweets_full <- rbind2(x = tweets_full, y = temp) -->
<!--   } -->

<!-- } -->

<!-- #test code -->
<!-- rm(temp) -->
<!-- temp <- search_fullarchive(q = query,  -->
<!--                                n = 1, -->
<!--                                fromDate = d1, -->
<!--                                toDate = d2, -->
<!--                                env_name = "historicalsearch", -->
<!--                                token = token) %>% -->
<!--       mutate(week = i, state = state.name[j]) -->
<!-- tweets_full -->
<!-- glimpse(temp) -->
<!-- query -->
<!-- d1 -->
<!-- d2 -->
<!-- token -->
<!-- master <- tibble() -->
<!-- master -->
<!-- search_fullarchive(q = query, n = 500, fromDate = 202009000000,  toDate = 202010110000, env_name = "historialsearch") -->
<!-- temp <- search_fullarchive(q = query, fromDate = d1,  toDate = d2, env_name = "historicalsearch") -->
<!-- master <- rbind2(x = master, y = temp) -->
<!-- temp -->
<!-- paste(dates_data$date[1+2*(39-1)],"0000",sep = "") -->
<!-- paste(state.name[1]," (covid OR mask OR mandate OR vaccine OR lockdown) -is:retweet",sep = "") -->
<!-- dates_data$date -->
<!-- ``` -->

<!-- ```{r reddit-scraping} -->
<!-- #search aggregator -->
<!-- paths_allowed("https://redditsearch.io/") -->

<!-- #site itself -->
<!-- paths_allowed("https://www.reddit.com/") -->

<!-- #grab links of all sites -->
<!-- samplelink <- "https://redditsearch.io/?term=texas%20covid&dataviz=false&aggs=false&subreddits=&searchtype=posts&search=true&start=1597982400&end=1598760000&size=100" -->

<!-- #get dates, convert to datetime for redditsearch -->
<!-- text_data <- " 2020-04-23 - 2020-05-05 -->
<!--  2020-05-07 - 2020-05-12 -->
<!--  2020-05-14 - 2020-05-19 -->
<!--  2020-05-21 - 2020-05-26 -->
<!--  2020-05-28 - 2020-06-02 -->
<!--  2020-06-04 - 2020-06-09 -->
<!--  2020-06-11 - 2020-06-16 -->
<!--  2020-06-18 - 2020-06-23 -->
<!--  2020-06-25 - 2020-06-30  -->
<!--  2020-07-02 - 2020-07-07 -->
<!--  2020-07-09 - 2020-07-14 -->
<!--  2020-07-16 - 2020-07-21 -->
<!--  2020-08-19 - 2020-08-31 -->
<!--  2020-09-02 - 2020-09-14 -->
<!--  2020-09-16 - 2020-09-28 -->
<!--  2020-09-30 - 2020-10-12 -->
<!--  2020-10-14 - 2020-10-26 -->
<!--  2020-10-28 - 2020-11-09 -->
<!--  2020-11-11 - 2020-11-23 -->
<!--  2020-11-25 - 2020-12-07 -->
<!--  2020-12-09 - 2020-12-21  -->
<!--  2021-01-06 - 2021-01-18 -->
<!--  2021-01-20 - 2021-02-01  -->
<!--  2021-02-03 - 2021-02-15 -->
<!--  2021-02-17 - 2021-03-01 -->
<!--  2021-03-03 - 2021-03-15 -->
<!--  2021-03-17 - 2021-03-29 -->
<!--  2021-04-14 - 2021-04-26 -->
<!--  2021-04-28 - 2021-05-10 -->
<!--  2021-05-12 - 2021-05-24  -->
<!--  2021-05-26 - 2021-06-07 -->
<!--  2021-06-09 - 2021-06-21 -->
<!--  2021-06-23 - 2021-07-05  -->
<!--  2021-07-21 - 2021-08-02 -->
<!--  2021-08-04 - 2021-08-16 -->
<!--  2021-08-18 - 2021-08-30 -->
<!--  2021-09-01 - 2021-09-13 -->
<!--  2021-09-15 - 2021-09-27 -->
<!--  2021-09-29 - 2021-10-11" -->
<!-- text_data <- text_data %>% -->
<!--   str_split("\n| - ") -->

<!-- text_data <- tibble(date = text_data) %>% -->
<!--   unnest() %>% #the query saves as seconds since 1970 -->
<!--   mutate(formatted_date = as.numeric(ymd(date)) * 24 * 60 * 60, start_end = "", week = NULL) -->

<!-- #add end/start defintion -->

<!-- for(i in 1:nrow(text_data)){ -->
<!--   if(i%%2 == 1){ -->
<!--     text_data$start_end[i] = "start" -->

<!--   } -->
<!--   else{ -->
<!--     text_data$start_end[i] = "end" -->
<!--   } -->
<!--   text_data$week[i] = as.integer(i/2) + i%%2 -->
<!-- } -->

<!-- #finished dates -->

<!-- #scrape URLS for search terms -->

<!-- samplelink <- "https://redditsearch.io/?term=texas%20covid&dataviz=false&aggs=false&subreddits=&searchtype=posts&search=true&start=1597982400&end=1598760000&size=100" -->

<!-- "askreddit,movies,music,todayilearned,science,politics,news,conservative" -->

<!-- for(i in 1:39){ -->
<!--   for(j in 1:50){ -->
<!--     #query text with state name -->
<!--     terms <- c("covid","mask","mandate","vaccine","lockdown") -->
<!--     query = c() -->
<!--     for(i in 1:length(terms)){ -->
<!--       query[i] <- paste("https://redditsearch.io/?term=",  -->
<!--                      state.name[j],  -->
<!--                      "%20", -->
<!--                      terms[i], -->
<!--                      "&dataviz=false&aggs=false&subreddits=movies,music,todayilearned,science,politics,news,conservative&searchtype=posts&search=true&start=", -->
<!--                      text_data$formatted_date[1+2*(i-1)], -->
<!--                      "&end=", -->
<!--                      text_data$formatted_date[2*i], -->
<!--                      "&size=100", -->
<!--                      sep = "") -->
<!--     } -->
<!--   } -->
<!-- } -->
<!-- #test code  -->
<!-- j = 4 -->
<!-- terms <- c("covid","mask","mandate","vaccine","lockdown") -->
<!-- query = c() -->
<!-- for(i in 1:length(terms)){ -->
<!--   query[i] <- paste("https://redditsearch.io/?term=",  -->
<!--                     state.name[j],"%20",terms[i], -->
<!--                      "&dataviz=false&aggs=false&subreddits=movies,music,todayilearned,science,politics,news,conservative&searchtype=posts&search=true&start=", -->
<!--                      text_data$formatted_date[1+2*(i-1)], -->
<!--                      "&end=", -->
<!--                      text_data$formatted_date[2*i], -->
<!--                      "&size=100", -->
<!--                      sep = "") -->
<!-- } -->
<!-- query -->
<!-- ``` -->



